<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>math/linear on</title><link>https://wiki.yigit.run/tags/math/linear/</link><description>Recent content in math/linear on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://wiki.yigit.run/tags/math/linear/index.xml" rel="self" type="application/rss+xml"/><item><title>Discrete Dynamical Systems</title><link>https://wiki.yigit.run/notes/Discrete-Dynamical-Systems/</link><pubDate>Mon, 21 Mar 2022 09:41:18 +0100</pubDate><guid>https://wiki.yigit.run/notes/Discrete-Dynamical-Systems/</guid><description>If you are dealing with a system whose state can be described by a list of numbers, you can describe the state using a [[/notes/Vectors|vector]], and the different states of the system can be shown using a [[/notes/Linear Transformations|linear transformation]].</description></item><item><title>Real Matrixes with Complex Eigenvalues</title><link>https://wiki.yigit.run/notes/Real-Matrixes-with-Complex-Eigenvalues/</link><pubDate>Sun, 20 Mar 2022 10:23:19 +0100</pubDate><guid>https://wiki.yigit.run/notes/Real-Matrixes-with-Complex-Eigenvalues/</guid><description>Given a $2 \times 2$ [[/notes/Matrixes|matrix]] in the form:
$$ A = \begin{bmatrix} a &amp;amp; -b \ b &amp;amp; a \end{bmatrix} $$</description></item><item><title>Complex Eigenvalues</title><link>https://wiki.yigit.run/notes/Complex-Eigenvalues/</link><pubDate>Sun, 20 Mar 2022 10:18:25 +0100</pubDate><guid>https://wiki.yigit.run/notes/Complex-Eigenvalues/</guid><description>The [[/notes/Eigenvalues|Eigenvalues]] of a given [[/notes/Matrixes|matrix]] don&amp;rsquo;t always have to be real, they can also be [[/notes/Complex Numbers|Complex Numbers]]. If $\lambda$ is an eigenvalue, than $\overline{\lambda}$ is also an eigenvalue with $\overline{v}$ as its [[/notes/Eigenvectors|eigenvector]].</description></item><item><title>Conditions For Diagonalization</title><link>https://wiki.yigit.run/notes/Conditions-For-Diagonalization/</link><pubDate>Sun, 20 Mar 2022 08:31:52 +0100</pubDate><guid>https://wiki.yigit.run/notes/Conditions-For-Diagonalization/</guid><description>To check if a [[/notes/Matrixes|matrix]] is [[/notes/Diagonalization|diagonalizable]], there are two conditions:
For an $n \times n$ matrix, the matrix must have $n$ [[/notes/Linear Indepence and Dependence|linearly independent]] [[/notes/Eigenvectors|Eigenvectors]]</description></item><item><title>Diagonalization</title><link>https://wiki.yigit.run/notes/Diagonalization/</link><pubDate>Sun, 20 Mar 2022 08:26:44 +0100</pubDate><guid>https://wiki.yigit.run/notes/Diagonalization/</guid><description>Diagonalization is a more specific form of [[/notes/Similar Matrices|Similar Matrices]], basically, diagonalization is the process of finding a [[/notes/Matrixes|matrix]] $D$ that is similar to $A$ and is a diagonal matrix.</description></item><item><title>Similar Matrices</title><link>https://wiki.yigit.run/notes/Similar-Matrices/</link><pubDate>Sun, 20 Mar 2022 08:15:27 +0100</pubDate><guid>https://wiki.yigit.run/notes/Similar-Matrices/</guid><description>Two [[/notes/Matrixes|matrices]] $A$ and $B$ are said to be similar if there exists an invertable matrix $P$ such that:
$$ A = PBP^{-1} $$</description></item><item><title>Algebraic and Geometric Multiplicity</title><link>https://wiki.yigit.run/notes/Algebraic-and-Geometric-Multiplicity/</link><pubDate>Sun, 20 Mar 2022 07:02:33 +0100</pubDate><guid>https://wiki.yigit.run/notes/Algebraic-and-Geometric-Multiplicity/</guid><description>The algebraic multiplicity of an [[/notes/Eigenvalues|eigenvalue]] $\lambda_0$ is the number of factors $(x - \lambda_0)$ in the [[/notes/Characteristic Equation|Characteristic Equation]] of the [[/notes/Matrixes|matrix]].</description></item><item><title>Characteristic Equation</title><link>https://wiki.yigit.run/notes/Characteristic-Equation/</link><pubDate>Sun, 20 Mar 2022 06:54:53 +0100</pubDate><guid>https://wiki.yigit.run/notes/Characteristic-Equation/</guid><description>A scalar $\lambda$ is a [[/notes/Matrixes|matrix]] $A$&amp;rsquo;s [[/notes/Eigenvalues|eigenvalue]] iff the [[/notes/Determinant|Determinant]] of $A - \lambda I$ is $0$.
The equation $det(A - \lambda I) = 0$ is called the characteristic equation</description></item><item><title>Properties of Eigenvalues</title><link>https://wiki.yigit.run/notes/Properties-of-Eigenvalues/</link><pubDate>Sun, 20 Mar 2022 06:52:08 +0100</pubDate><guid>https://wiki.yigit.run/notes/Properties-of-Eigenvalues/</guid><description>[[/notes/Eigenvalues|Eigenvalues]] have several properties:
[[/notes/Eigenvectors|Eigenvectors]] that correspond to different eigenvalues are linearly independent
This means that $n \times n$ [[/notes/Matrixes|matrix]] can have at most $n$ different eigenvalues.</description></item><item><title>Eigenspace</title><link>https://wiki.yigit.run/notes/Eigenspace/</link><pubDate>Sun, 20 Mar 2022 06:15:40 +0100</pubDate><guid>https://wiki.yigit.run/notes/Eigenspace/</guid><description>The [[/notes/Subspace|Subspace]] of [[/notes/Eigenvectors|Eigenvectors]] that have the [[/notes/Eigenvalues|eigenvalue]] $\lambda$ is called the eigenspace of that eigenvalue and is represented by $E_\lambda$.</description></item><item><title>Eigenvalues</title><link>https://wiki.yigit.run/notes/Eigenvalues/</link><pubDate>Sun, 20 Mar 2022 06:11:04 +0100</pubDate><guid>https://wiki.yigit.run/notes/Eigenvalues/</guid><description>The eigenvalue of an [[/notes/Eigenvectors|eigenvector]] $w$ is basically how much the [[/notes/Linear Transformations|linear transformation]] scales the [[/notes/Vectors|vector]]. An eigenvector can only have one eigenvalue, however, there are infinitely many eigenvectors for a given eigenvalue, since all the multiples of the eigenvector $w$ has an eigenvalue of $\lambda$ as well.</description></item><item><title>Eigenvectors</title><link>https://wiki.yigit.run/notes/Eigenvectors/</link><pubDate>Sun, 20 Mar 2022 06:03:22 +0100</pubDate><guid>https://wiki.yigit.run/notes/Eigenvectors/</guid><description>Eigenvectors are [[/notes/Vectors|Vectors]] which remain in the same span when a [[/notes/Linear Transformations|linear transformation]] $A$ is applied to them. However, they can be scaled up or down.</description></item><item><title>Determinants and Row Operations</title><link>https://wiki.yigit.run/notes/Determinants-and-Row-Operations/</link><pubDate>Sun, 06 Mar 2022 10:57:42 +0100</pubDate><guid>https://wiki.yigit.run/notes/Determinants-and-Row-Operations/</guid><description>When you do [[/notes/Elementary Row Operations|Elementary Row Operations]] on a [[/notes/Matrixes|matrix]], the [[/notes/Determinant|Determinant]] of that matrix changes depending on the row operation:</description></item><item><title>Determinants of Special Matrices</title><link>https://wiki.yigit.run/notes/Determinants-of-Special-Matrices/</link><pubDate>Sun, 06 Mar 2022 10:53:45 +0100</pubDate><guid>https://wiki.yigit.run/notes/Determinants-of-Special-Matrices/</guid><description>It is much easier to calculate the [[/notes/Determinant|determinants]] of some [[/notes/Special Matrices|Special Matrices]]. Especially the diagonal square ones. The determinant of any square diagonal matrice is the multiplication of its diagonal entries.</description></item><item><title>Calculating Determinants</title><link>https://wiki.yigit.run/notes/Calculating-Determinants/</link><pubDate>Sun, 06 Mar 2022 10:45:50 +0100</pubDate><guid>https://wiki.yigit.run/notes/Calculating-Determinants/</guid><description>Calculating [[/notes/Determinant|determinants]] with the concept of [[/notes/Cofactor|cofactor]] is relatively straight forward. Given an $n \times m$ matrix $A$, the determinant is defined by one of the following formulas:</description></item><item><title>Determinant</title><link>https://wiki.yigit.run/notes/Determinant/</link><pubDate>Sun, 06 Mar 2022 10:32:54 +0100</pubDate><guid>https://wiki.yigit.run/notes/Determinant/</guid><description>The determinant of a [[/notes/Matrixes|matrix]] is how much that [[/notes/Linear Transformations|linear transformation]] changes the space that a unit square takes. Since a unit square can be infinitely small, a determinant therefore gives us by what factor $c$ any given matrix multiplies any shape&amp;rsquo;s area.</description></item><item><title>Cofactor</title><link>https://wiki.yigit.run/notes/Cofactor/</link><pubDate>Sun, 06 Mar 2022 10:31:49 +0100</pubDate><guid>https://wiki.yigit.run/notes/Cofactor/</guid><description>The cofactor $C_{ij}$ of a matrix $A$ is the [[/notes/Determinant|determinant]] of the [[/notes/Minor Matrix|Minor Matrix]] $A_{ij}$.</description></item><item><title>Minor Matrix</title><link>https://wiki.yigit.run/notes/Minor-Matrix/</link><pubDate>Sun, 06 Mar 2022 10:27:38 +0100</pubDate><guid>https://wiki.yigit.run/notes/Minor-Matrix/</guid><description>The minor [[/notes/Matrixes|matrix]] $A_{ij}$ of a matrix $A$, is the matrix denoted by deleting the $i^{th}$ row and $j^{th}$ column from the matrix $A$.</description></item><item><title>Rank</title><link>https://wiki.yigit.run/notes/Rank/</link><pubDate>Sun, 27 Feb 2022 12:11:51 +0100</pubDate><guid>https://wiki.yigit.run/notes/Rank/</guid><description>The rank of a [[/notes/Matrixes|matrix]] $A$, denoted by $Rank(A)$, is the [[/notes/Dimension|Dimension]] of the [[/notes/Column Space|Column Space]] of $A$.
If $A$ is a matrix of size $m \times n$, then $Rank(A) + dim(Nul(A)) = n$</description></item><item><title>Dimension</title><link>https://wiki.yigit.run/notes/Dimension/</link><pubDate>Sun, 27 Feb 2022 12:07:35 +0100</pubDate><guid>https://wiki.yigit.run/notes/Dimension/</guid><description>The dimension of a [[/notes/Subspace|Subspace]] $dim W$ is the number of vectors in the [[/notes/Basis Vectors|basis]].</description></item><item><title>Coordinate Vectors</title><link>https://wiki.yigit.run/notes/Coordinate-Vectors/</link><pubDate>Sun, 27 Feb 2022 11:58:15 +0100</pubDate><guid>https://wiki.yigit.run/notes/Coordinate-Vectors/</guid><description>When you have a [[/notes/Subspace|Subspace]] $W$ which is the span of some [[/notes/Basis Vectors|Basis Vectors]] $\mathcal{B}$. You can calculate the coordinates of a vector $\mathbf{x}$ relative to the subspace $W$, which is represented by $[\mathbf{x}]\mathcal{B}$ by solving the linear system $A[\mathbf{x}]\mathcal{B} = \mathbf{x}$.</description></item><item><title>Parametric Vector Form</title><link>https://wiki.yigit.run/notes/Parametric-Vector-Form/</link><pubDate>Sun, 27 Feb 2022 11:11:01 +0100</pubDate><guid>https://wiki.yigit.run/notes/Parametric-Vector-Form/</guid><description>In order to convert a linear system into parametric vector form, first the matrix must be converted to reduced echeleon form.</description></item><item><title>Basis of the null space</title><link>https://wiki.yigit.run/notes/Basis-of-the-null-space/</link><pubDate>Sun, 27 Feb 2022 11:05:41 +0100</pubDate><guid>https://wiki.yigit.run/notes/Basis-of-the-null-space/</guid><description>The basis of the [[/notes/Null Space|Null Space]] of a matrix $A$ is determined using the following method.
Create a linear system $Ax = 0$ Convert it into [[/notes/Parametric Vector Form|Parametric Vector Form]] The column vectors in this parametric vector form are the basis of the null space.</description></item><item><title>Basis of a column space</title><link>https://wiki.yigit.run/notes/Basis-of-a-column-space/</link><pubDate>Sun, 27 Feb 2022 11:02:39 +0100</pubDate><guid>https://wiki.yigit.run/notes/Basis-of-a-column-space/</guid><description>The [[/notes/Basis Vectors|Basis Vectors]] of a [[/notes/Column Space|Column Space]] are determined using the following method:
The [[/notes/Matrixes|matrix]] is row reduced so that it is in [[/notes/Echeleon Forms|echeleon form]] The columns in the original matrix that have a pivot point in the reduced matrix act as the basis vectors of the column space</description></item><item><title>Basis Vectors</title><link>https://wiki.yigit.run/notes/Basis-Vectors/</link><pubDate>Sun, 27 Feb 2022 10:58:38 +0100</pubDate><guid>https://wiki.yigit.run/notes/Basis-Vectors/</guid><description>The basis vectors of a [[/notes/Subspace|Subspace]] $W$ is a set of vectors that are:
[[/notes/Linear Indepence and Dependence|Linearly independent]] [[/notes/Vector Span|Span]] $W$ A subspace can have more than one basis</description></item><item><title>Null Space</title><link>https://wiki.yigit.run/notes/Null-Space/</link><pubDate>Sun, 27 Feb 2022 10:48:34 +0100</pubDate><guid>https://wiki.yigit.run/notes/Null-Space/</guid><description>The null space $Nul A$ of a matrix $m \times n$ $A$ is the [[/notes/Subspace|Subspace]] that is generated by the [[/notes/Solutions of Linear Systems|solution of the linear system]] $Ax = 0$.</description></item><item><title>Column Space</title><link>https://wiki.yigit.run/notes/Column-Space/</link><pubDate>Sun, 27 Feb 2022 10:47:02 +0100</pubDate><guid>https://wiki.yigit.run/notes/Column-Space/</guid><description>The column space of a matrix, $Col A$ is the [[/notes/Subspace|Subspace]] that is spanned by the columns of an $m \times n$ matrix $A$.</description></item><item><title>Subspace</title><link>https://wiki.yigit.run/notes/Subspace/</link><pubDate>Sun, 27 Feb 2022 10:32:49 +0100</pubDate><guid>https://wiki.yigit.run/notes/Subspace/</guid><description>A subspace of $\mathbb{R}^n$ is a subset $W \in \mathbb{R}^n$. For a subset to be considered a subspace, it must conform to the following conditions:</description></item><item><title>Vectors</title><link>https://wiki.yigit.run/notes/Vectors/</link><pubDate>Sun, 27 Feb 2022 10:27:42 +0100</pubDate><guid>https://wiki.yigit.run/notes/Vectors/</guid><description>Vectors are basically [[/notes/Matrixes|Matrixes]] with one column or one row. They are used to represent a point, in space, or more oftenly a line with a direction with magnitude.</description></item><item><title>Matrix Transformation</title><link>https://wiki.yigit.run/notes/Matrix-Transformation/</link><pubDate>Sat, 26 Feb 2022 12:14:25 +0100</pubDate><guid>https://wiki.yigit.run/notes/Matrix-Transformation/</guid><description>Matrix transformations are equations of the form:
$$ T(x) = Ax $$
Where $A$ is a matrix. The domain of the transformation is the size of the vector that is inputted into the transformation, while the codomain is the size of the vector that transformation outputs:</description></item><item><title>Check if a matrix is invertible</title><link>https://wiki.yigit.run/notes/Check-if-a-matrix-is-invertible/</link><pubDate>Sat, 26 Feb 2022 07:32:31 +0100</pubDate><guid>https://wiki.yigit.run/notes/Check-if-a-matrix-is-invertible/</guid><description>A [/notes/Matrixes|matrix] of size $n \times n$ is [[/notes/Inverse Matrix|invertable]] if and only if one of the conditions below hold:</description></item><item><title>Inverse Matrix</title><link>https://wiki.yigit.run/notes/Inverse-Matrix/</link><pubDate>Sat, 26 Feb 2022 07:24:28 +0100</pubDate><guid>https://wiki.yigit.run/notes/Inverse-Matrix/</guid><description>Inverse [[/notes/Matrixes|Matrixes]] are basically [[/notes/Linear Transformations|Linear Transformations]] that can reverse the effects of a matrix. Finding the inverse of a matrix is done through this method:</description></item><item><title>Matrix Transpose</title><link>https://wiki.yigit.run/notes/Matrix-Transpose/</link><pubDate>Sat, 26 Feb 2022 04:35:40 +0100</pubDate><guid>https://wiki.yigit.run/notes/Matrix-Transpose/</guid><description>The transpose of a [[/notes/Matrixes|matrix]] is basically the matrix but turned around. The transpose of a matrix $n \times m$ is equal to a matrix of size $m \times n$, in which, rows become columns.</description></item><item><title>Matrix Multiplication Row-Column Rule</title><link>https://wiki.yigit.run/notes/Matrix-Multiplication-Row-Column-Rule/</link><pubDate>Sat, 26 Feb 2022 04:10:53 +0100</pubDate><guid>https://wiki.yigit.run/notes/Matrix-Multiplication-Row-Column-Rule/</guid><description>When you [[/notes/Matrix Multiplication|multiply]] two matrices, $A$ and $B$, and only want one entry($ij$) in the result matrice $C$, we can use the formula:</description></item><item><title>Matrix Multiplication</title><link>https://wiki.yigit.run/notes/Matrix-Multiplication/</link><pubDate>Sat, 26 Feb 2022 03:43:30 +0100</pubDate><guid>https://wiki.yigit.run/notes/Matrix-Multiplication/</guid><description>For a [[/notes/Matrixes|matrix]] multiplication to be valid, you need two vectors $A$ and $B$ such that $A$ is of size $m \times n$ and $B$ of size $n \times p$ and the multiplication must occur in the order $AB$, and results in a matrix of size $m \times p$.</description></item><item><title>Special Matrices</title><link>https://wiki.yigit.run/notes/Special-Matrices/</link><pubDate>Sat, 26 Feb 2022 03:24:08 +0100</pubDate><guid>https://wiki.yigit.run/notes/Special-Matrices/</guid><description>Zero matrix A matrix with zeros instead of all its values:
$$ \begin{bmatrix} 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \end{bmatrix} $$</description></item><item><title>Matrixes</title><link>https://wiki.yigit.run/notes/Matrixes/</link><pubDate>Sat, 26 Feb 2022 03:21:37 +0100</pubDate><guid>https://wiki.yigit.run/notes/Matrixes/</guid><description>Matrixes are basically multiple [[/notes/Vectors|Vectors]] side by side, or, more accurately, vectors are smaller matrices, $n \times 1$ to be precise.</description></item><item><title>Rotational Transformations</title><link>https://wiki.yigit.run/notes/Rotational-Transformations/</link><pubDate>Sat, 26 Feb 2022 02:54:19 +0100</pubDate><guid>https://wiki.yigit.run/notes/Rotational-Transformations/</guid><description>[[/notes/Linear Transformations|Linear Transformations]] alter the coordinate system such that the origin stays the same. So, they are useful for representing rotations using matrices.</description></item><item><title>Linear Transformations and Unit Vectors</title><link>https://wiki.yigit.run/notes/Linear-Transformations-and-Unit-Vectors/</link><pubDate>Sat, 26 Feb 2022 02:52:34 +0100</pubDate><guid>https://wiki.yigit.run/notes/Linear-Transformations-and-Unit-Vectors/</guid><description>When doing [[/notes/Linear Transformations|Linear Transformations]], the columns of the matrix $A$ actually represent the unit vectors of the new coordinate systems.</description></item><item><title>Linear Transformations</title><link>https://wiki.yigit.run/notes/Linear-Transformations/</link><pubDate>Sat, 26 Feb 2022 01:34:45 +0100</pubDate><guid>https://wiki.yigit.run/notes/Linear-Transformations/</guid><description>A transformation with domain $n$ and codomain $m$, which is made using an $n \times m$ matrix, is represented by $\mathbb{R}^n \to \mathbb{R}^m$.</description></item><item><title>Solution of Non-Homogenous Systems</title><link>https://wiki.yigit.run/notes/Solution-of-Non-Homogenous-Systems/</link><pubDate>Wed, 23 Feb 2022 12:47:43 +0100</pubDate><guid>https://wiki.yigit.run/notes/Solution-of-Non-Homogenous-Systems/</guid><description>A [[/notes/Consistent and Inconsistent Systems|consistent]] non-homogenous system $Ax=b$&amp;rsquo;s solution is $x=x_p + x_h$. In which
$x_p$ is a special solution for the homogenous system $Ax=b$ $x_h$ is the solution for homogenous system $Ax=0$ !</description></item><item><title>Homogenous Linear System Solutions</title><link>https://wiki.yigit.run/notes/Homogenous-Linear-System-Solutions/</link><pubDate>Wed, 23 Feb 2022 12:39:14 +0100</pubDate><guid>https://wiki.yigit.run/notes/Homogenous-Linear-System-Solutions/</guid><description>A [[/notes/Homogenous Linear Systems|homogenous linear system]] can have another solution than the [[/notes/Trivial Solution|Trivial Solution]] iff it has a [[/notes/Free and Basic Variables|free variable]].</description></item><item><title>Trivial Solution</title><link>https://wiki.yigit.run/notes/Trivial-Solution/</link><pubDate>Wed, 23 Feb 2022 12:38:03 +0100</pubDate><guid>https://wiki.yigit.run/notes/Trivial-Solution/</guid><description>A trivial solution of a linear system of equations is the 0 vector.</description></item><item><title>Linear Indepence and Dependence</title><link>https://wiki.yigit.run/notes/Linear-Indepence-and-Dependence/</link><pubDate>Wed, 23 Feb 2022 01:07:48 +0100</pubDate><guid>https://wiki.yigit.run/notes/Linear-Indepence-and-Dependence/</guid><description>A set of vectors are linearly dependent if:
$$ \begin{bmatrix} v_1 &amp;amp; v_2 &amp;amp; v_3 &amp;amp; &amp;hellip; &amp;amp; v_n \end{bmatrix} x = 0 $$</description></item><item><title>Homogenous Linear Systems</title><link>https://wiki.yigit.run/notes/Homogenous-Linear-Systems/</link><pubDate>Tue, 22 Feb 2022 07:14:07 +0100</pubDate><guid>https://wiki.yigit.run/notes/Homogenous-Linear-Systems/</guid><description>A system $Ax = b$ is homogenous if $b=0$, non-homogenous otherwise.
A homogenous system is always [[/notes/Consistent and Inconsistent Systems|consistent]]</description></item><item><title>Augmented Matrix and Vector Matrix Multiplication</title><link>https://wiki.yigit.run/notes/Augmented-Matrix-and-Vector-Matrix-Multiplication/</link><pubDate>Tue, 22 Feb 2022 06:46:34 +0100</pubDate><guid>https://wiki.yigit.run/notes/Augmented-Matrix-and-Vector-Matrix-Multiplication/</guid><description>An [[/notes/Augmented Matrix|Augmented Matrix]] can also be written using [[/notes/Matrix-Vector Product|Matrix-Vector Product]] like below: $$ \left[\begin{array}{ccc|c}
2 &amp;amp; 0 &amp;amp; 1 &amp;amp; 3\</description></item><item><title>Matrix-Vector Product</title><link>https://wiki.yigit.run/notes/Matrix-Vector-Product/</link><pubDate>Tue, 22 Feb 2022 06:35:56 +0100</pubDate><guid>https://wiki.yigit.run/notes/Matrix-Vector-Product/</guid><description>The product of a matrix $A$ and a [[/notes/Vectors|vector]] $v$ can be written as:
$$ Ax = \begin{bmatrix} a_1 &amp;amp; a_2 &amp;amp; &amp;hellip; &amp;amp; a_n \end{bmatrix} \begin{bmatrix} x_1 \ x_2 \ &amp;hellip; \ x_n \end{bmatrix} = x_1a_1 + x_2a_2 + &amp;hellip; + x_na_n $$</description></item><item><title>Vector Span</title><link>https://wiki.yigit.run/notes/Vector-Span/</link><pubDate>Tue, 22 Feb 2022 06:29:52 +0100</pubDate><guid>https://wiki.yigit.run/notes/Vector-Span/</guid><description>The span of $n$ [[/notes/Vectors|Vectors]] is basically all the [[/notes/Linear Combinations|Linear Combinations]] of those vectors. And is written as:
$$ Span{v_1, v_2, &amp;hellip;, v_3} $$</description></item><item><title>Linear Combinations</title><link>https://wiki.yigit.run/notes/Linear-Combinations/</link><pubDate>Tue, 22 Feb 2022 06:13:07 +0100</pubDate><guid>https://wiki.yigit.run/notes/Linear-Combinations/</guid><description>Given the [[/notes/Vectors|Vectors]] $v_1,v_2,&amp;hellip;v_n$ in $\mathbb{R}^n$, and scalars $c_1,c_2,&amp;hellip;,c_n$, the linear combination of these vectors, $y$, is:
$$ y = c_1v_1 + c_2v_2 + c_3v_3 + &amp;hellip; + c_nv_n $$</description></item><item><title>Free and Basic Variables</title><link>https://wiki.yigit.run/notes/Free-and-Basic-Variables/</link><pubDate>Tue, 22 Feb 2022 04:28:22 +0100</pubDate><guid>https://wiki.yigit.run/notes/Free-and-Basic-Variables/</guid><description>A variable in a linear system can either be basic, or free.
A variable is basic if when reduced to [[/notes/Echeleon Forms|echeleon form]], it acts as a pivot.</description></item><item><title>Consistent and Inconsistent Systems</title><link>https://wiki.yigit.run/notes/Consistent-and-Inconsistent-Systems/</link><pubDate>Tue, 22 Feb 2022 04:17:00 +0100</pubDate><guid>https://wiki.yigit.run/notes/Consistent-and-Inconsistent-Systems/</guid><description>Whether a system of linear equations has a [[/notes/Solutions of Linear Systems|solution]] is decided by whether the system is consistent or not.</description></item><item><title>Reduced Echeleon Forms</title><link>https://wiki.yigit.run/notes/Reduced-Echeleon-Forms/</link><pubDate>Tue, 22 Feb 2022 04:13:13 +0100</pubDate><guid>https://wiki.yigit.run/notes/Reduced-Echeleon-Forms/</guid><description>Reduced echeleon forms are a specialized form of [[/notes/Echeleon Forms|Echeleon Forms]] which is:
$$ \begin{bmatrix} 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; * &amp;amp; 0 \ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; * &amp;amp; 0 \ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \end{bmatrix} $$</description></item><item><title>Echeleon Forms</title><link>https://wiki.yigit.run/notes/Echeleon-Forms/</link><pubDate>Tue, 22 Feb 2022 04:03:44 +0100</pubDate><guid>https://wiki.yigit.run/notes/Echeleon-Forms/</guid><description>Echeleon forms are special matrices such that they are in the form:
$$ \begin{bmatrix} 0 &amp;amp; P &amp;amp; * &amp;amp; * &amp;amp; * \ 0 &amp;amp; 0 &amp;amp; P &amp;amp; * &amp;amp; * \ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; P \ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \end{bmatrix} $$</description></item><item><title>Elementary Row Operations</title><link>https://wiki.yigit.run/notes/Elementary-Row-Operations/</link><pubDate>Fri, 11 Feb 2022 09:54:28 +0100</pubDate><guid>https://wiki.yigit.run/notes/Elementary-Row-Operations/</guid><description>Elementary row operations change an [[/notes/Augmented Matrix|Augmented Matrix]] while keeping it [[/notes/Equivalent Systems|equivalent]].
Interchange rows Interchange the rows bby.
Multiply by factor The factor cannot be 0.</description></item><item><title>Augmented Matrix</title><link>https://wiki.yigit.run/notes/Augmented-Matrix/</link><pubDate>Fri, 11 Feb 2022 09:52:53 +0100</pubDate><guid>https://wiki.yigit.run/notes/Augmented-Matrix/</guid><description>![[images/A7E0F393-DBB5-46CE-B6F8-8458BA9FD6E1.jpeg]]</description></item><item><title>Equivalent Systems</title><link>https://wiki.yigit.run/notes/Equivalent-Systems/</link><pubDate>Fri, 11 Feb 2022 09:51:52 +0100</pubDate><guid>https://wiki.yigit.run/notes/Equivalent-Systems/</guid><description>Two linear systems are equivalent if they have the same [[/notes/Solutions of Linear Systems|solutions]].</description></item><item><title>Solutions of Linear Systems</title><link>https://wiki.yigit.run/notes/Solutions-of-Linear-Systems/</link><pubDate>Fri, 11 Feb 2022 09:50:08 +0100</pubDate><guid>https://wiki.yigit.run/notes/Solutions-of-Linear-Systems/</guid><description>A linear system can have three possible states:
![[images/180F9890-1E39-41E1-A9D1-420FAB46682F.jpeg|300]]
In order to solve a linear system, the system is converted to [[/notes/Reduced Echeleon Forms|reduced echeleon form]], and then to [[/notes/Parametric Vector Form|parametric form]].</description></item></channel></rss>