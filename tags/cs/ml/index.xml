<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cs/ml on</title><link>https://wiki.yigit.run/tags/cs/ml/</link><description>Recent content in cs/ml on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://wiki.yigit.run/tags/cs/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Gaussian Distribution</title><link>https://wiki.yigit.run/notes/Gaussian-Distribution/</link><pubDate>Sun, 16 Oct 2022 02:26:16 +0200</pubDate><guid>https://wiki.yigit.run/notes/Gaussian-Distribution/</guid><description>The gaussian distribution, also called the normal distribution occurs in many places in life. It is defined like so:
$$ p(x | \mu, \sigma) = \frac{1}{\sqrt{2\pi \sigma^2}}exp(-\frac{(x-\mu)^2}{2\sigma^2}) $$</description></item><item><title>Discriminative vs Generative Models</title><link>https://wiki.yigit.run/notes/Discriminative-vs-Generative-Models/</link><pubDate>Sun, 25 Sep 2022 05:34:41 +0200</pubDate><guid>https://wiki.yigit.run/notes/Discriminative-vs-Generative-Models/</guid><description>There are two approaches to [[notes/Classification|classification]], discriminative and generative models.
Discriminative Models Discrimintive models aim to classify objects by just knowing the [[notes/Posterior Probability]].</description></item><item><title>Misclassification Cost Matrix</title><link>https://wiki.yigit.run/notes/Misclassification-Cost-Matrix/</link><pubDate>Sun, 25 Sep 2022 05:20:41 +0200</pubDate><guid>https://wiki.yigit.run/notes/Misclassification-Cost-Matrix/</guid><description>The [[notes/Misclassification Cost|Misclassification Cost]] for each class can be placed into a matrix of size $C \times C$ where $C$ is the number of classes.</description></item><item><title>Conditional And Total Risk</title><link>https://wiki.yigit.run/notes/Conditional-And-Total-Risk/</link><pubDate>Sun, 25 Sep 2022 05:09:37 +0200</pubDate><guid>https://wiki.yigit.run/notes/Conditional-And-Total-Risk/</guid><description>The conditional risk of assigning an object $x$ to the class $\omega_i$ is ( given the [[notes/Misclassification Cost|missclassification costs]]): $$ l^i(x) = \sum_{j=1}^C \lambda_{j,i}p(\omega_j|x) $$ This means the average risk(expectation) over a region $\Omega_i$ is: $$ $$ $$ \begin{align*} r^i &amp;amp;= \int_{\Omega_i} l^i(x)p(x)dx \ &amp;amp;= \int_{\Omega_i} \sum_{j=1}^C \lambda_{j,i}p(\omega_j|x)p(x)dx \end{align*} $$</description></item><item><title>Empirical Risk</title><link>https://wiki.yigit.run/notes/Empirical-Risk/</link><pubDate>Sun, 25 Sep 2022 04:13:33 +0200</pubDate><guid>https://wiki.yigit.run/notes/Empirical-Risk/</guid><description>When you [[notes/Classification|classify]] the points in the dataset as $D = {(x_i, \omega_i)}_{i=1}^N$ and you assign each of the points to a class $\hat{\omega_i}$.</description></item><item><title>Misclassification Cost</title><link>https://wiki.yigit.run/notes/Misclassification-Cost/</link><pubDate>Sun, 25 Sep 2022 04:09:27 +0200</pubDate><guid>https://wiki.yigit.run/notes/Misclassification-Cost/</guid><description>In order to optimize the [[notes/Classification|classification]] model to avoid certain kinds of [[notes/Classification Error|error]]. Misclassification costs can be set during its training.</description></item><item><title>Bayes Error</title><link>https://wiki.yigit.run/notes/Bayes-Error/</link><pubDate>Sun, 25 Sep 2022 04:05:13 +0200</pubDate><guid>https://wiki.yigit.run/notes/Bayes-Error/</guid><description>Bayes error is the minimum [[notes/Classification Error|classification error]] you can have for the true distributions of the data. Regardless of how big your dataset is, you can&amp;rsquo;t get lower than this error.</description></item><item><title>Classification Error</title><link>https://wiki.yigit.run/notes/Classification-Error/</link><pubDate>Sun, 25 Sep 2022 04:01:13 +0200</pubDate><guid>https://wiki.yigit.run/notes/Classification-Error/</guid><description>The [[notes/Classification|classification]] error is the overall error we make with a given model for a given dataset. It is calculated using the formula:</description></item><item><title>Errors in a more general sense</title><link>https://wiki.yigit.run/notes/Errors-in-a-more-general-sense/</link><pubDate>Sun, 25 Sep 2022 03:55:16 +0200</pubDate><guid>https://wiki.yigit.run/notes/Errors-in-a-more-general-sense/</guid><description>For a two-type [[notes/Classification|classification]] problem: There are two different types of errors: $\varepsilon_1$ and $\varepsilon_2$. You can calculate them like so:</description></item><item><title>Bayes Theorem</title><link>https://wiki.yigit.run/notes/Bayes-Theorem/</link><pubDate>Wed, 21 Sep 2022 12:36:21 +0200</pubDate><guid>https://wiki.yigit.run/notes/Bayes-Theorem/</guid><description>Bayes&amp;rsquo; theorem is used to describe the probability of an event. It is defined as:
$$ p(y | x) = \frac{p(x|y)p(y)}{p(x)} $$</description></item><item><title>Decision Boundaries</title><link>https://wiki.yigit.run/notes/Decision-Boundaries/</link><pubDate>Wed, 21 Sep 2022 12:32:05 +0200</pubDate><guid>https://wiki.yigit.run/notes/Decision-Boundaries/</guid><description>Decision boundaries are boundaries that split up classes. A data point is classified based on which part of the boundary it falls to.</description></item><item><title>Posterior Probability</title><link>https://wiki.yigit.run/notes/Posterior-Probability/</link><pubDate>Wed, 21 Sep 2022 12:29:43 +0200</pubDate><guid>https://wiki.yigit.run/notes/Posterior-Probability/</guid><description>Posterior probability is the probability that a given data points belongs to a class. It is often represented by $p(\omega_1|x)$ where $\omega_1$ is a group and $x$ is a data point.</description></item><item><title>Classification</title><link>https://wiki.yigit.run/notes/Classification/</link><pubDate>Wed, 21 Sep 2022 12:27:00 +0200</pubDate><guid>https://wiki.yigit.run/notes/Classification/</guid><description>Classification is the problem of guessing to which class it belongs(often represented by the symbol $\omega$) given a data point. For this problem, it is quite common to calculate the [[notes/Posterior Probability|posterior probability]] of a data point for each class and classifying that point as the group with the largest probability.</description></item><item><title>Training and Testing Sets</title><link>https://wiki.yigit.run/notes/Training-And-Testing-Sets/</link><pubDate>Wed, 21 Sep 2022 12:22:40 +0200</pubDate><guid>https://wiki.yigit.run/notes/Training-And-Testing-Sets/</guid><description>When training a machine learning model, it is common practice to split your dataset into two sets. Training and testing sets.</description></item></channel></rss>