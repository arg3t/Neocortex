<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cs/algorithms on</title><link>https://wiki.yigit.run/tags/cs/algorithms/</link><description>Recent content in cs/algorithms on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://wiki.yigit.run/tags/cs/algorithms/index.xml" rel="self" type="application/rss+xml"/><item><title>Exchange Argument</title><link>https://wiki.yigit.run/notes/Exchange-Argument/</link><pubDate>Wed, 25 Jan 2023 01:41:10 +0100</pubDate><guid>https://wiki.yigit.run/notes/Exchange-Argument/</guid><description>The exchange argument is a proof method used to prove the optimality of [[notes/Greedy Algorithm|Greedy Algorithms]]. The proof is based on the idea that given an optimal solution with inversions, commpared to the the greedy solution, fixing those inversions brings the optimal solution closer to the greedy solution whilst preserving optimality.</description></item><item><title>Greedy Stays Ahead</title><link>https://wiki.yigit.run/notes/Greedy-Stays-Ahead/</link><pubDate>Wed, 25 Jan 2023 01:25:07 +0100</pubDate><guid>https://wiki.yigit.run/notes/Greedy-Stays-Ahead/</guid><description>Greedy stays ahead is a technique to prove that a solution generated by a greedy algorithm is optimal. A general step by step approach to prove an algorithm&amp;rsquo;s correctness can be summarized as such.</description></item><item><title>Greedy Algorithm</title><link>https://wiki.yigit.run/notes/Greedy-Algorithm/</link><pubDate>Wed, 25 Jan 2023 01:03:11 +0100</pubDate><guid>https://wiki.yigit.run/notes/Greedy-Algorithm/</guid><description>A greedy algorithm([[notes/Greedy Method|Greedy Method]]) is an algorithmic approach that follows the problem-solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum.</description></item><item><title>Kruskal's Algorithm</title><link>https://wiki.yigit.run/notes/Kruskals-Algorithm/</link><pubDate>Mon, 24 Jan 2022 04:28:32 +0100</pubDate><guid>https://wiki.yigit.run/notes/Kruskals-Algorithm/</guid><description>In order to create the minimum spanning tree of a [[notes/Graphs|graph]], the kruskal&amp;rsquo;s algorithm takes the following approach:
Save each of the edges into a priority queue Create $n$ clusters, each containing one of the vertices.</description></item><item><title>Greedy Method</title><link>https://wiki.yigit.run/notes/Greedy-Method/</link><pubDate>Mon, 24 Jan 2022 03:40:54 +0100</pubDate><guid>https://wiki.yigit.run/notes/Greedy-Method/</guid><description>The greedy problem solving approach is the approach of picking the most locally optimal choice in each step. This design pattern often doesn&amp;rsquo;t produce the globally optimal solution to a problem but yields a reasonably quick solution that is close enough to the globally optimal answer.</description></item><item><title>Checking if a graph is a DAG</title><link>https://wiki.yigit.run/notes/Checking-if-a-graph-is-a-DAG/</link><pubDate>Mon, 24 Jan 2022 03:34:06 +0100</pubDate><guid>https://wiki.yigit.run/notes/Checking-if-a-graph-is-a-DAG/</guid><description>Checking whether a [[notes/Graphs|graph]] is a [[notes/Directed Acyclic Graphs|Directed Acyclic Graphs]] is a trivial problem. First off, we need to know that for a graph to be a DAG, there must exit at least one vertice with $0$ incoming edges, otherwise, it cannot be a DAG(go figure out why yourself, you nitwit).</description></item><item><title>Breadth First Search</title><link>https://wiki.yigit.run/notes/Breadth-First-Search/</link><pubDate>Mon, 24 Jan 2022 02:33:01 +0100</pubDate><guid>https://wiki.yigit.run/notes/Breadth-First-Search/</guid><description>Breadth first search, or [[notes/Breadth-First Traversal|Breadth First Traversal]], similar to [[notes/Depth-First Search|Depth-First Search]] is a traversal algorithm for [[notes/Graphs|Graphs]]. I works in rounds, by splitting the graph into levels strarting from the start node $s$.</description></item><item><title>DFS and Spanning Trees</title><link>https://wiki.yigit.run/notes/DFS-and-Spanning-Trees/</link><pubDate>Mon, 24 Jan 2022 02:30:59 +0100</pubDate><guid>https://wiki.yigit.run/notes/DFS-and-Spanning-Trees/</guid><description>When running [[notes/Depth-First Search|Depth-First Search]] on a [[notes/Graphs|graph]], a spanning tree that is rooted at the algorithm&amp;rsquo;s start node, $s$ is formed.</description></item><item><title>Depth-First Search</title><link>https://wiki.yigit.run/notes/Depth-First-Search/</link><pubDate>Mon, 24 Jan 2022 02:15:36 +0100</pubDate><guid>https://wiki.yigit.run/notes/Depth-First-Search/</guid><description>Depth first search is a traversal algorithm for [[notes/Graphs|Graphs]]. In DFS, edges are explored until we hit a dead-end where we can go to an unvisited vertice and we backtrack until the last decision point after that.</description></item><item><title>Red-Black Tree Deletion</title><link>https://wiki.yigit.run/notes/Red-Black-Tree-Deletion/</link><pubDate>Wed, 05 Jan 2022 08:43:21 +0300</pubDate><guid>https://wiki.yigit.run/notes/Red-Black-Tree-Deletion/</guid><description>After deleting from the [[notes/Red-Black Trees|Red-Black Trees]] just like a [[notes/Deleting From A Binary Search Tree|BST]], the promoted node is colored black.</description></item><item><title>Red-Black Tree Insertions</title><link>https://wiki.yigit.run/notes/Red-Black-Tree-Insertions/</link><pubDate>Wed, 05 Jan 2022 08:42:32 +0300</pubDate><guid>https://wiki.yigit.run/notes/Red-Black-Tree-Insertions/</guid><description>After inserting into a [[notes/Red-Black Trees|red black tree]] just like a [[notes/Binary Search Trees|BST]], the newly inserted node is colored red.</description></item><item><title>AVL Tree Operations</title><link>https://wiki.yigit.run/notes/AVL-Tree-Operations/</link><pubDate>Sun, 02 Jan 2022 10:46:07 +0300</pubDate><guid>https://wiki.yigit.run/notes/AVL-Tree-Operations/</guid><description>Insertion After inserting into an AVL Tree, which works just like a [[notes/Binary Tree|Binary Tree]], we insert the node into the tree, we traverse upwards from the inserted node, checking if the AVL property is broken at any point in the tree.</description></item><item><title>Binary Tree Restructuring</title><link>https://wiki.yigit.run/notes/Binary-Tree-Restructuring/</link><pubDate>Sun, 02 Jan 2022 10:33:43 +0300</pubDate><guid>https://wiki.yigit.run/notes/Binary-Tree-Restructuring/</guid><description>Sometimes, when creating a [[notes/Balanced Binary Search Trees|balanced tree]], [[notes/Binary Tree Rotations|rotations]] may not be sufficient. In this case, a technique called trinode structuring comes in, which is essentially one or two rotations combined.</description></item><item><title>AVL Trees</title><link>https://wiki.yigit.run/notes/AVL-Trees/</link><pubDate>Sun, 02 Jan 2022 10:30:38 +0300</pubDate><guid>https://wiki.yigit.run/notes/AVL-Trees/</guid><description>Keeping a [[notes/Binary Tree|Binary Tree]] [[notes/Balanced Binary Search Trees|balanced]] is crucial to sustain its performance, and AVLs trees ensure that by enforcing one simple rule to these trees:</description></item><item><title>Binary Tree Traversal</title><link>https://wiki.yigit.run/notes/Binary-Tree-Traversal/</link><pubDate>Sun, 02 Jan 2022 09:44:41 +0300</pubDate><guid>https://wiki.yigit.run/notes/Binary-Tree-Traversal/</guid><description>With recursion With recursion, it pretty straightforward, here is an example java code:
1 2 3 4 5 6 7 public static void traverse(BinaryTree t, List&amp;lt;Integer&amp;gt; l){ if(t == null) return; traverse(t.</description></item><item><title>Binary Tree Rotations</title><link>https://wiki.yigit.run/notes/Binary-Tree-Rotations/</link><pubDate>Sat, 01 Jan 2022 03:14:57 +0100</pubDate><guid>https://wiki.yigit.run/notes/Binary-Tree-Rotations/</guid><description>A rotation of a [[notes/Binary Tree|Binary Tree]] node is basically replacing a node with one of its children. In order to sustain the search tree property of a binary tree however, when replacing a node with its left child, the host becomes its right child, and the reverse applies when replacing wit its right child.</description></item><item><title>Deleting From A Binary Search Tree</title><link>https://wiki.yigit.run/notes/Deleting-From-A-Binary-Search-Tree/</link><pubDate>Sat, 01 Jan 2022 03:08:29 +0100</pubDate><guid>https://wiki.yigit.run/notes/Deleting-From-A-Binary-Search-Tree/</guid><description>Deleting from a [[notes/Binary Search Trees|binary search tree]] can be a tricky process since we want to sustain the BST property of the tree upon deletion.</description></item><item><title>Hash Functions</title><link>https://wiki.yigit.run/notes/Hash-Functions/</link><pubDate>Fri, 31 Dec 2021 12:54:13 +0300</pubDate><guid>https://wiki.yigit.run/notes/Hash-Functions/</guid><description>Hash functions are functions that take in data, or a stream of data and output a value that is set in size.</description></item><item><title>Hash Collision Handling</title><link>https://wiki.yigit.run/notes/Hash-Collision-Handling/</link><pubDate>Fri, 31 Dec 2021 01:21:45 +0300</pubDate><guid>https://wiki.yigit.run/notes/Hash-Collision-Handling/</guid><description>Since we apply [[notes/Hash Compression|Hash Compression]] in [[notes/Hash Tables|Hash Tables]], hash collisions may occur often, so handling the collisions becomes critical in order to sustain an efficient [[notes/Big-Oh Notation|time complexity]].</description></item><item><title>Hash Compression</title><link>https://wiki.yigit.run/notes/Hash-Compression/</link><pubDate>Fri, 31 Dec 2021 01:03:33 +0300</pubDate><guid>https://wiki.yigit.run/notes/Hash-Compression/</guid><description>It can be very useful to compress a hash value so that it becomes smaller, there are some methods to achieve this:</description></item><item><title>Quick Select</title><link>https://wiki.yigit.run/notes/Quick-Select/</link><pubDate>Mon, 27 Dec 2021 01:17:19 +0300</pubDate><guid>https://wiki.yigit.run/notes/Quick-Select/</guid><description>Quick Select is an algorithm similar to [[notes/Quick Sort|Quick Sort]], except it does not sort a given but finds the nth smallest element in an array in $O(n)$ time.</description></item><item><title>In-Place Quick Sort</title><link>https://wiki.yigit.run/notes/In-Place-Quick-Sort/</link><pubDate>Sat, 25 Dec 2021 02:40:50 +0100</pubDate><guid>https://wiki.yigit.run/notes/In-Place-Quick-Sort/</guid><description>Implementing [[notes/Quick Sort|Quick Sort]] in place allows us to decrease the space-complexity of the algorithm and make it slightly more efficient.</description></item><item><title>Radix Sort</title><link>https://wiki.yigit.run/notes/Radix-Sort/</link><pubDate>Fri, 24 Dec 2021 04:28:25 +0100</pubDate><guid>https://wiki.yigit.run/notes/Radix-Sort/</guid><description>Radix sort is a non-comparison based sorting algorithm, it allows us to do stable sorting in a fast manner for elements that are more complicated than integers.</description></item><item><title>Bucket Sort</title><link>https://wiki.yigit.run/notes/Bucket-Sort/</link><pubDate>Fri, 24 Dec 2021 04:18:32 +0100</pubDate><guid>https://wiki.yigit.run/notes/Bucket-Sort/</guid><description>Bucket sort is a non-comparison based sorting algorithm. It has the restriction that the integers in a list are in the range $[0, N]$.</description></item><item><title>Quick Sort</title><link>https://wiki.yigit.run/notes/Quick-Sort/</link><pubDate>Thu, 23 Dec 2021 12:52:48 +0300</pubDate><guid>https://wiki.yigit.run/notes/Quick-Sort/</guid><description>Quick sort is an algorithm that splits the array into three parts according to a pivot value, L, E, G, array with the elements that are less then, equal to, or greater than the pivot respectively.</description></item><item><title>Quick Sort Pivot Selection</title><link>https://wiki.yigit.run/notes/Quick-Sort-Pivot-Selection/</link><pubDate>Thu, 23 Dec 2021 01:05:53 +0300</pubDate><guid>https://wiki.yigit.run/notes/Quick-Sort-Pivot-Selection/</guid><description>The most important aspect in a quick sort algorithm is how the pivot is chosen, the pivot decides how well-balanced the recursion tree of the algorithm turns out to be.</description></item><item><title>Analysis of Merge Sort</title><link>https://wiki.yigit.run/notes/Analysis-of-Merge-Sort/</link><pubDate>Tue, 21 Dec 2021 08:35:42 +0300</pubDate><guid>https://wiki.yigit.run/notes/Analysis-of-Merge-Sort/</guid><description>Calculating the [[notes/Big-Oh Notation|Big-Oh Complexity]] of [[notes/Merge Sort|Merge Sort]] is not a difficult task. There are two ways to do it, one being more intuitive and the other being more mathematical.</description></item><item><title>Merging in Merge Sort</title><link>https://wiki.yigit.run/notes/Merging-in-Merge-Sort/</link><pubDate>Tue, 21 Dec 2021 07:59:57 +0300</pubDate><guid>https://wiki.yigit.run/notes/Merging-in-Merge-Sort/</guid><description>After the recursive calls return two sorted lists, you need to merge them. Since they are both sorted, merging them into a new sorted list can be done in $O(n)$ complexity.</description></item><item><title>Merge Sort</title><link>https://wiki.yigit.run/notes/Merge-Sort/</link><pubDate>Tue, 21 Dec 2021 04:05:02 +0100</pubDate><guid>https://wiki.yigit.run/notes/Merge-Sort/</guid><description>Merge sort is a sorting algorithm based on a divide and conquer approach. Basically in merge-sort, a list is halved and sorted and then the sorted parts are then [[notes/Merging in Merge Sort|merged]].</description></item><item><title>Heap Sort</title><link>https://wiki.yigit.run/notes/Heap-Sort/</link><pubDate>Tue, 21 Dec 2021 02:54:31 +0100</pubDate><guid>https://wiki.yigit.run/notes/Heap-Sort/</guid><description>Heap sort is a faster version of [[notes/Insertion Sort|Insertion Sort]], optimised by implementing a [[notes/Heap|Heap]] into the [[notes/Priority Queue|Priority Queue]] so that the insertion and removal stages take $O(nlog(n))$ time.</description></item><item><title>Bottom-Up Heap Construction</title><link>https://wiki.yigit.run/notes/Bottom-Up-Heap-Construction/</link><pubDate>Tue, 21 Dec 2021 02:25:16 +0100</pubDate><guid>https://wiki.yigit.run/notes/Bottom-Up-Heap-Construction/</guid><description>Constructing a heap from scratch by inserting each element into the heap takes $O(nlog(n))$ time. However, when you have a list with $2^n-1$ elements, you can construct a heap with those elements in $O(n)$ time using the following steps.</description></item><item><title>Insertion Sort on a Partially Sorted List</title><link>https://wiki.yigit.run/notes/Insertion-Sort-on-a-Partially-Sorted-List/</link><pubDate>Tue, 21 Dec 2021 02:14:06 +0100</pubDate><guid>https://wiki.yigit.run/notes/Insertion-Sort-on-a-Partially-Sorted-List/</guid><description>When you have a partially sorted list, you can change the definition of the [[notes/Priority Queue|Priority Queue]] you use in [[notes/Insertion Sort|Insertion Sort]] so that the new elements are first added to the end of the queue.</description></item><item><title>Insertion Sort</title><link>https://wiki.yigit.run/notes/Insertion-Sort/</link><pubDate>Tue, 21 Dec 2021 01:51:28 +0100</pubDate><guid>https://wiki.yigit.run/notes/Insertion-Sort/</guid><description>Insertion sort, is an algorithm based on [[notes/Priority Queue|Priority Queue]]s. In insertion sort, you insert the elements that you want to sort into the priority queue in a sorted order.</description></item><item><title>Selection Sort</title><link>https://wiki.yigit.run/notes/Selection-Sort/</link><pubDate>Tue, 21 Dec 2021 01:45:34 +0100</pubDate><guid>https://wiki.yigit.run/notes/Selection-Sort/</guid><description>Selection sort is a sorting algorithm based on [[notes/Priority Queue|Priority Queue]]s. In order to sort using selection sort, the items are inserted into a priority queue with their values as keys in the order they are currently at (i.</description></item><item><title>Breadth First Traversal</title><link>https://wiki.yigit.run/notes/Breadth-First-Traversal/</link><pubDate>Tue, 14 Dec 2021 20:32:21 +0100</pubDate><guid>https://wiki.yigit.run/notes/Breadth-First-Traversal/</guid><description>In breadth first [[notes/Tree Traversal|Tree Traversal]], the nodes at the same depth([[notes/Tree Terminology|Important Tree Terminology]]) are visited first, before visiting the nodes at higher depths.</description></item><item><title>Down-Heap Bubbling</title><link>https://wiki.yigit.run/notes/Down-Heap-Bubbling/</link><pubDate>Tue, 14 Dec 2021 20:32:21 +0100</pubDate><guid>https://wiki.yigit.run/notes/Down-Heap-Bubbling/</guid><description>Down-Heap bubbling is basically the reverse of [[notes/Up-Heap Bubbling|Heap Bubbling]], when the heap-order property of a heap is broken due to a node that is not a leaf, you need to apply down-heap bubbling.</description></item><item><title>Pre-Order Tree Traversal</title><link>https://wiki.yigit.run/notes/Pre-Order-Traversal/</link><pubDate>Tue, 14 Dec 2021 20:32:21 +0100</pubDate><guid>https://wiki.yigit.run/notes/Pre-Order-Traversal/</guid><description>This is a method of depth first [[notes/Tree Traversal|Tree Traversal]] in which we visit a node first and then visit its children, recursively.</description></item><item><title>Empirical Analysis</title><link>https://wiki.yigit.run/notes/Empirical-Analysis/</link><pubDate>Tue, 16 Nov 2021 21:07:41 +0100</pubDate><guid>https://wiki.yigit.run/notes/Empirical-Analysis/</guid><description>Empirical analysis is the process of running an algorithm with different input sizes and plotting/analyzin the change in input times overtime.</description></item></channel></rss>